{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import sklearn.model_selection as skm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dane\n",
    "data = fetch_openml(data_id=31)\n",
    "\n",
    "# Utwórz DataFrame z danych i etykiet\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "\n",
    "# Dodaj kolumnę z etykietami\n",
    "df['target'] = pd.Series(data.target, name='target')\n",
    "\n",
    "df = pd.get_dummies(df)\n",
    "df.replace({True: 1, False: 0}, inplace=True)\n",
    "y = df['target_good'] \n",
    "X = df.drop(['target_good', 'target_bad'], axis=1)\n",
    "# zbiór testowy i treningowy \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=335723)\n",
    "kfold = skm.KFold(5,\n",
    "                  random_state=335723,\n",
    "                  shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#model regresji logistycznej\n",
    "model_lr = LogisticRegression(penalty = None , max_iter = 300).fit(X_train,y_train) \n",
    "\n",
    "y_pred = model_lr.predict(X_test)\n",
    "y_proba = model_lr.predict_proba(X_test)[:, 1] \n",
    "\n",
    "#miary treningowe\n",
    "y_pred_train = model_lr.predict(X_train)\n",
    "y_proba_train = model_lr.predict_proba(X_train)[:, 1] \n",
    "\n",
    "accuracy = accuracy_score(y_train, y_pred_train)\n",
    "precision = precision_score(y_train, y_pred_train)\n",
    "recall = recall_score(y_train, y_pred_train)\n",
    "auc_value = roc_auc_score(y_train, y_proba_train)\n",
    "\n",
    "print(f'Dokładność trenigowa: {accuracy:.4f}')\n",
    "print(f'Precyzja treningowa: {precision:.4f}')\n",
    "print(f'Czułość trenigowa: {recall:.4f}')\n",
    "print(f'Wartość AUC treningowa: {auc_value:.4f}')\n",
    "\n",
    "#miary testowe\n",
    "y_pred = model_lr.predict(X_test)\n",
    "y_proba = model_lr.predict_proba(X_test)[:, 1] \n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "auc_value = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "print(f'Dokładność: {accuracy:.4f}')\n",
    "print(f'Precyzja: {precision:.4f}')\n",
    "print(f'Czułość: {recall:.4f}')\n",
    "print(f'Wartość AUC: {auc_value:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model regresji logistycznej z regularyzacją L1\n",
    "model_l1 = LogisticRegression(penalty='l1') \n",
    "param_grid = {\n",
    "    'C': [100,10, 5, 2, 1, 0.5, 0.1, 0.01, 0.005],\n",
    "    'solver' : ['liblinear','saga'],\n",
    "    'max_iter' : [100,300]\n",
    "}\n",
    "\n",
    "\n",
    "# Utwórz obiekt GridSearchCV\n",
    "grid_search = GridSearchCV(model_l1, param_grid, cv=kfold, scoring='accuracy')\n",
    "\n",
    "# Dopasuj model do danych treningowych\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Wydrukuj najlepsze parametry\n",
    "print(\"Najlepsze parametry:\", grid_search.best_params_)\n",
    "\n",
    "#miary treningowe\n",
    "y_pred_train = grid_search.predict(X_train)\n",
    "y_proba_train = grid_search.predict_proba(X_train)[:, 1] \n",
    "\n",
    "accuracy = accuracy_score(y_train, y_pred_train)\n",
    "precision = precision_score(y_train, y_pred_train)\n",
    "recall = recall_score(y_train, y_pred_train)\n",
    "auc_value = roc_auc_score(y_train, y_proba_train)\n",
    "\n",
    "print(f'Dokładność trenigowa: {accuracy:.4f}')\n",
    "print(f'Precyzja treningowa: {precision:.4f}')\n",
    "print(f'Czułość trenigowa: {recall:.4f}')\n",
    "print(f'Wartość AUC treningowa: {auc_value:.4f}')\n",
    "\n",
    "#miary testowe\n",
    "y_pred = grid_search.predict(X_test)\n",
    "y_proba = grid_search.predict_proba(X_test)[:, 1] \n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "auc_value = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "print(f'Dokładność: {accuracy:.4f}')\n",
    "print(f'Precyzja: {precision:.4f}')\n",
    "print(f'Czułość: {recall:.4f}')\n",
    "print(f'Wartość AUC: {auc_value:.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#model regresji logistycznej z regularyzacją L2\n",
    "model_l2 = LogisticRegression(penalty='l2') \n",
    "param_grid = {\n",
    "    'C': [100,10, 5, 2, 1, 0.5, 0.1, 0.01, 0.005],\n",
    "    'solver' : ['lbfgs','liblinear','newton-cg','newton-cholesky',\n",
    "                'sag', 'saga'],\n",
    "    'max_iter' : [100,300]\n",
    "}\n",
    "\n",
    "\n",
    "# Utwórz obiekt GridSearchCV\n",
    "grid_search = GridSearchCV(model_l2, param_grid, cv=kfold, scoring='accuracy')\n",
    "\n",
    "# Dopasuj model do danych treningowych\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Wydrukuj najlepsze parametry\n",
    "print(\"Najlepsze parametry:\", grid_search.best_params_)\n",
    "\n",
    "\n",
    "#miary treningowe\n",
    "y_pred_train = grid_search.predict(X_train)\n",
    "y_proba_train = grid_search.predict_proba(X_train)[:, 1] \n",
    "\n",
    "accuracy = accuracy_score(y_train, y_pred_train)\n",
    "precision = precision_score(y_train, y_pred_train)\n",
    "recall = recall_score(y_train, y_pred_train)\n",
    "auc_value = roc_auc_score(y_train, y_proba_train)\n",
    "\n",
    "print(f'Dokładność trenigowa: {accuracy:.4f}')\n",
    "print(f'Precyzja treningowa: {precision:.4f}')\n",
    "print(f'Czułość trenigowa: {recall:.4f}')\n",
    "print(f'Wartość AUC treningowa: {auc_value:.4f}')\n",
    "\n",
    "#miary testowe\n",
    "y_pred = grid_search.predict(X_test)\n",
    "y_proba = grid_search.predict_proba(X_test)[:, 1] \n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "auc_value = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "print(f'Dokładność: {accuracy:.4f}')\n",
    "print(f'Precyzja: {precision:.4f}')\n",
    "print(f'Czułość: {recall:.4f}')\n",
    "print(f'Wartość AUC: {auc_value:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 na raz\n",
    "model_lr = LogisticRegression(penalty = None , max_iter = 300).fit(X_train,y_train) \n",
    "model_l1 = LogisticRegression(penalty='l1' ,C =  0.5, max_iter =  100, solver= 'liblinear').fit(X_train, y_train)\n",
    "model_l2 = LogisticRegression(penalty='l2',C= 10, max_iter= 100, solver = 'newton-cg').fit(X_train, y_train) \n",
    "\n",
    "pred1 = model_lr.predict_proba(X_test)\n",
    "pred2 = model_l1.predict_proba(X_test)\n",
    "pred3 = model_l2.predict_proba(X_test)\n",
    "\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, pred1[:,1])\n",
    "plt.plot(fpr,tpr,label=\"None, AUC=\"+str(round(roc_auc_score(y_test, pred1[:,1]), 4)))\n",
    "fpr, tpr, thresholds = roc_curve(y_test, pred2[:,1])\n",
    "plt.plot(fpr,tpr,label=\"l1, max_depth=3, AUC=\"+str(round(roc_auc_score(y_test, pred2[:,1]), 4)))\n",
    "fpr, tpr, thresholds = roc_curve(y_test, pred3[:,1])\n",
    "plt.plot(fpr,tpr,label=\"l2, AUC=\"+str(round(roc_auc_score(y_test, pred3[:,1]), 4)))\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Krzywa ROC')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs_l1 = pd.Series(model_l1.coef_[0], index=model_l1.feature_names_in_)\n",
    "non_zero_coefs_l1 = coefs_l1[np.abs(coefs_l1) >= 1e-2]\n",
    "\n",
    "print(f'Ilość współczynników: {len(coefs_l1)}, ilość (prawie) niezerowych współczynników: {len(non_zero_coefs_l1)}')\n",
    "\n",
    "X_train_reduced, X_test_reduced = X_train[non_zero_coefs_l1.index], X_test[non_zero_coefs_l1.index]\n",
    "nazwy_kolumn = X_train_reduced.columns\n",
    "print(nazwy_kolumn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_linear = SVC(kernel = 'linear', C=10)\n",
    "svm_linear.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "#miary treningowe\n",
    "y_pred_train = svm_linear.predict(X_train)\n",
    "\n",
    "accuracy = accuracy_score(y_train, y_pred_train)\n",
    "precision = precision_score(y_train, y_pred_train)\n",
    "recall = recall_score(y_train, y_pred_train)\n",
    "auc_value = roc_auc_score(y_train, y_proba_train)\n",
    "\n",
    "print(f'Dokładność trenigowa: {accuracy:.4f}')\n",
    "print(f'Precyzja treningowa: {precision:.4f}')\n",
    "print(f'Czułość trenigowa: {recall:.4f}')\n",
    "print(f'Wartość AUC treningowa: {auc_value:.4f}')\n",
    "\n",
    "#miary testowe\n",
    "y_pred = grid_search.predict(X_test)\n",
    "y_proba = grid_search.predict_proba(X_test)[:, 1] \n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "auc_value = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "print(f'Dokładność: {accuracy:.4f}')\n",
    "print(f'Precyzja: {precision:.4f}')\n",
    "print(f'Czułość: {recall:.4f}')\n",
    "print(f'Wartość AUC: {auc_value:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import RocCurveDisplay\n",
    "RocCurveDisplay.from_estimator(svm_linear, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
